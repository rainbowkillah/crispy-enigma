# M2 Milestone: AI Gateway Integration

**Date:** 2026-02-07
**Status:** IN PROGRESS
**Issues:** #25-#31
**Depends on:** M0 (Foundation), M1 (Chat + Sessions)

---

## Objective

Route all Workers AI calls through AI Gateway with tenant-scoped model routing, usage metrics, and budget enforcement. Validate the streaming integration that was built in M1 works end-to-end with real AI Gateway responses.

---

## M0-M1 Verification (Pre-M2)

Verified on 2026-02-07:

- **36 tests passing** across 12 test files (up from 32 at M1 close due to 4 new ai-gateway tests)
- **TypeScript compiles** with zero errors
- **Lint:** 15 pre-existing style errors (no-explicit-any), non-blocking
- All M0 deliverables (#3-#14) intact
- All M1 deliverables (#15-#24) intact
- AI Gateway package (`packages/ai/src/gateway.ts`) already has foundational implementation:
  - `runGatewayChat()` function with gateway metadata, model fallback, streaming support
  - SSE stream parsing (`aiStreamToTokenStream`)
  - Text response extraction from multiple response formats
  - Integrated into `/chat` endpoint in `apps/worker-api/src/index.ts`

---

## Deliverables

| Issue | Title | Priority | Status |
|-------|-------|----------|--------|
| #25 | Gateway integration spike - prove connectivity + configuration | CRITICAL | In progress |
| #26 | Implement model routing (tenant config chooses models) | HIGH | Complete |
| #27 | Add per-route model override options | MEDIUM | Complete |
| #28 | Implement budget/limits hooks (token limits) | HIGH | Complete |
| #29 | Add observability hooks (latency, status, tokens in/out) | HIGH | Complete |
| #30 | Document AI Gateway integration in docs/ai-gateway.md | MEDIUM | Complete |
| #31 | Document fallback behavior for API changes | MEDIUM | Complete |

---

## M2 Progress Snapshot (2026-02-07)

- **40 tests passing** across 13 test files
- Added model override, allow-list enforcement, token budgets, usage metrics
- New docs: `docs/ai-gateway.md`, `docs/ai-gateway-fallbacks.md`

---

## Dev Smoke Test Findings (2026-02-07)

- `mrrainbowsmoke` dev server started with remote bindings.
- `GET /health` succeeded with expected model + fallback.
- `POST /chat` with `stream:false` returned `ai_error` (502).
- `POST /chat` with `stream:true` returned SSE `event: error` + `event: done` and usage payload (tokensIn + latencyMs, no tokensOut).
- `rainbowsmokeofficial` dev server failed to start remote proxy: Cloudflare API error on `/workers/subdomain/edge-preview` (see Wrangler log under `~/.config/.wrangler/logs/`).
- `rainbowsmokeofficial` `--local` dev server started; `GET /health` succeeded. Both `stream:false` and `stream:true` `/chat` calls returned `ai_error` (AI binding not supported locally), SSE `done` included usage payload with tokensIn + latencyMs.

---

## Acceptance Criteria

From `docs/plan.md`:

1. All model calls use `env.AI.run()` with `gateway` parameter (verified by gateway logs)
2. Tenant-specific model selection works via `gateway.metadata: { tenantId }`
3. Usage metrics (latency, tokens in/out, cost) are recorded in gateway analytics

---

## What Already Exists (M1 Carryover)

The following M2-related code was already built during M1:

### `packages/ai/src/gateway.ts`
- `runGatewayChat()` - Main gateway function with tenant-scoped metadata
- `buildGatewayOptions()` - Constructs `{ gateway: { id, metadata } }` options
- `aiStreamToTokenStream()` - Converts AI response streams to token streams
- `extractTextResponse()` - Handles multiple response object shapes
- Model fallback logic (primary -> fallback on error)

### `apps/worker-api/src/index.ts`
- `/chat` endpoint already calls `runGatewayChat()` with:
  - `tenant.tenantId` and `tenant.aiGatewayId`
  - Model from `tenant.aiModels.chat` (with env override)
  - Fallback model support
  - Gateway metadata (traceId, sessionId, route)

### `tests/ai-gateway.test.ts` (4 tests)
- Gateway routing with tenant metadata
- Tenant-specific model selection
- Stream flag passthrough
- Fallback model behavior

---

## Implementation Plan

### Phase 1: Gateway Spike (#25)

**Goal:** Prove end-to-end AI Gateway connectivity with a real Cloudflare account.

Tasks:
- [ ] Deploy minimal worker to a staging tenant with real AI binding
- [ ] Verify `env.AI.run()` with `gateway` param routes through AI Gateway
- [ ] Confirm `metadata.tenantId` appears in gateway dashboard logs
- [ ] Validate streaming works through gateway (TTFB, token delivery)
- [ ] Document findings in spike report

**Preview domain format (dev):** `<worker-name>-<env>.<tenant>.workers.dev`  
Example: `bluey-ai-worker-dev.mrrainbowsmoke.workers.dev`

**Risk:** HIGH - This validates the core assumption. If streaming through gateway behaves differently than expected, the SSE integration may need rework.

### Phase 2: Model Routing & Overrides (#26, #27)

**Goal:** Complete model selection logic with per-route overrides.

Tasks:
- [x] Verify tenant config model selection works (already implemented)
- [x] Add per-request `modelId` field to chat request schema
- [x] Implement model override precedence: request > env > tenant config
- [x] Add model allow-list validation per tenant (optional, via featureFlags)
- [x] Add tests for model override behavior

**What's already done:**
- `tenant.aiModels.chat` selection works
- `env.MODEL_ID` override works
- Fallback model logic works

### Phase 3: Observability & Budget Hooks (#28, #29)

**Goal:** Capture usage metrics and enforce budget limits.

Tasks:
- [x] Add latency tracking to `runGatewayChat()` (start/end timestamps)
- [x] Extract token counts from AI response (if available in response headers/body)
- [x] Create `GatewayUsageMetrics` type (latency, tokensIn, tokensOut, modelId, tenantId)
- [x] Add usage metrics callback/hook to `runGatewayChat()` options
- [x] Implement basic token budget check (per-tenant daily/monthly limit)
- [x] Add `tokenBudget` field to tenant config schema
- [x] Store usage counts in KV (tenant-scoped)
- [x] Add tests for metrics collection and budget enforcement
- [x] Add usage-related response headers (`x-tokens-in`, `x-tokens-out`, `x-tokens-total`, `x-model-id`)

**Risk:** MEDIUM - Token counts may not be available in streaming responses. May need to estimate or defer detailed attribution to M4.

### Phase 4: Documentation (#30, #31)

**Goal:** Document AI Gateway patterns and fallback behavior.

Tasks:
- [x] Create `docs/ai-gateway.md` with:
  - How `env.AI.run()` with gateway param works
  - Tenant metadata strategy
  - Model selection precedence
  - Streaming behavior details
  - Error handling patterns
- [x] Document fallback behavior:
  - Model fallback (primary -> fallback)
  - Gateway unavailable fallback
  - Streaming error recovery
  - API version change mitigation
- [x] Update `docs/architecture.md` with AI Gateway flow
- [x] Add M2 ADR for model selection precedence

---

## Risk Assessment

| Risk | Level | Mitigation |
|------|-------|------------|
| AI Gateway streaming integration | HIGH | Start with spike (#25) to validate early |
| Token count extraction from streams | MEDIUM | Defer detailed attribution to M4, use estimates |
| Error handling mid-stream | MEDIUM | Robust error event handling in SSE |
| Model selection complexity | LOW | Simple precedence: request > env > tenant config |

---

## Quality Gates

- All existing 36 tests continue to pass
- New tests added for: model overrides, observability hooks, budget enforcement
- Target: 45+ tests at M2 completion
- TypeScript compiles with zero errors
- Documentation complete and accurate
- No new technical debt introduced

---

## References

- `packages/ai/src/gateway.ts` - AI Gateway wrapper
- `apps/worker-api/src/index.ts` - Chat endpoint integration
- `tests/ai-gateway.test.ts` - Gateway tests
- `docs/plan.md` - Master plan (M2 section)
- `docs/M2-PREP.md` - M2 readiness assessment
- `docs/streaming.md` - SSE contract (M1)
- `docs/ai-gateway.md` - AI Gateway integration guide
- `docs/ai-gateway-fallbacks.md` - AI Gateway fallback behavior
- Cloudflare AI Gateway docs: https://developers.cloudflare.com/ai-gateway/
